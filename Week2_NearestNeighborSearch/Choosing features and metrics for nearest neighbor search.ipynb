{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n",
      "[INFO] sframe.cython.cy_server: SFrame v2.0.1 started. Logging /tmp/sframe_server_1468347662.log\n",
      "INFO:sframe.cython.cy_server:SFrame v2.0.1 started. Logging /tmp/sframe_server_1468347662.log\n"
     ]
    }
   ],
   "source": [
    "import sframe                            # see below for install instruction\n",
    "import matplotlib.pyplot as plt          # plotting\n",
    "import numpy as np                       # dense matrices\n",
    "from scipy.sparse import csr_matrix      # sparse matrices\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the dataset\n",
    "\n",
    "We will be using the same dataset of Wikipedia pages that we used in the Machine Learning Foundations course (Course 1). Each element of the dataset consists of a link to the wikipedia article, the name of the person, and the text of the article (in lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki = sframe.SFrame('_data/people_wiki.gl/')\n",
    "wiki = wiki.add_row_number()             # add row number, starting at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract word count vectors\n",
    "\n",
    "For your convenience, we extracted the word count vectors from the dataset. The vectors are packaged in a sparse matrix, where the i-th row gives the word count vectors for the i-th document. Each column corresponds to a unique word appearing in the dataset. The mapping between words and integer indices are given in people_wiki_map_index_to_word.gl.\n",
    "\n",
    "To load in the word count vectors, define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "    \n",
    "    return csr_matrix( (data, indices, indptr), shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count = load_sparse_csr('_data/people_wiki_word_count.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_index_to_word = sframe.SFrame('_data/people_wiki_map_index_to_word.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Extracting word count vectors yourself. We provide the pre-computed word count vectors to minimize potential compatibility issues. You are free to experiment with other tools to compute the word count vectors yourself. A good place to start is **sklearn.CountVectorizer**. Note. Due to variations in [tokenization](https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)) and other factors, your word count vectors may differ from the ones we provide. For the purpose the assessment, we ask you to use the vectors from people_wiki_word_count.npz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest neighbors using word count vectors\n",
    "\n",
    "Let's start by finding the nearest neighbors of the Barack Obama page using the word count vectors to represent the articles and Euclidean distance to measure distance. For this, we will use scikit-learn's implementation of k-nearest neighbors. We first create an instance of the NearestNeighbor class, specifying the model parameters. Then we call the fit() method to attach the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', leaf_size=30, metric='euclidean',\n",
       "         metric_params=None, n_jobs=1, n_neighbors=5, p=2, radius=1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model = NearestNeighbors(metric='euclidean', algorithm='brute')\n",
    "model.fit(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+--------------+\n",
      "|   id  |              URI              |     name     |\n",
      "+-------+-------------------------------+--------------+\n",
      "| 35817 | <http://dbpedia.org/resour... | Barack Obama |\n",
      "+-------+-------------------------------+--------------+\n",
      "+-------------------------------+\n",
      "|              text             |\n",
      "+-------------------------------+\n",
      "| barack hussein obama ii br... |\n",
      "+-------------------------------+\n",
      "[? rows x 4 columns]\n",
      "Note: Only the head of the SFrame is printed. This SFrame is lazily evaluated.\n",
      "You can use sf.materialize() to force materialization.\n"
     ]
    }
   ],
   "source": [
    "print wiki[wiki['name'] == 'Barack Obama']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which locates Obama's article at index 35817.\n",
    "\n",
    "Let us run the k-nearest neighbor algorithm with Obama's article. Since the NearestNeighbor class expects a vector, we pass the 35817th row of word_count vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances, indices = model.kneighbors(word_count[35817], n_neighbors=10) # 1st arg: word count vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,  33.07567082,  34.39476704,  36.15245497,\n",
       "         36.16628264,  36.33180425,  36.40054945,  36.49657518,\n",
       "         36.63331817,  36.95943723]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35817, 24478, 28447, 35357, 14754, 13229, 31423, 22745, 36364,\n",
       "         9210]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query returns the indices of and distances to the 10 nearest neighbors. To display the indices and distances together with the article name, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,  33.07567082,  34.39476704,  36.15245497,\n",
       "        36.16628264,  36.33180425,  36.40054945,  36.49657518,\n",
       "        36.63331817,  36.95943723])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------+---------------+\n",
      "|   id  |            name            |    distance   |\n",
      "+-------+----------------------------+---------------+\n",
      "| 35817 |        Barack Obama        |      0.0      |\n",
      "| 24478 |         Joe Biden          | 33.0756708171 |\n",
      "| 28447 |       George W. Bush       | 34.3947670438 |\n",
      "| 35357 |      Lawrence Summers      | 36.1524549651 |\n",
      "| 14754 |        Mitt Romney         | 36.1662826401 |\n",
      "| 13229 |      Francisco Barrio      | 36.3318042492 |\n",
      "| 31423 |       Walter Mondale       | 36.4005494464 |\n",
      "| 22745 | Wynn Normington Hugh-Jones | 36.4965751818 |\n",
      "| 36364 |         Don Bonker         |  36.633318168 |\n",
      "|  9210 |        Andy Anstett        | 36.9594372252 |\n",
      "+-------+----------------------------+---------------+\n",
      "[10 rows x 3 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neighbors = sframe.SFrame({'distance':distances.flatten(), 'id':indices.flatten()})\n",
    "\n",
    "print wiki.join(neighbors, on='id').sort('distance')[['id','name','distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
